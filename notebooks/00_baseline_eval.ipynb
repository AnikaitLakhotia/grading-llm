{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline evaluation\n",
    "\n",
    "This notebook runs two baselines on the processed dataset:\n",
    "1. TF-IDF + LogisticRegression (classical baseline)\n",
    "2. Mock LLM via `PromptRunner` (simulated LLM baseline)\n",
    "\n",
    "Metrics: Quadratic Weighted Kappa (QWK), ±1 accuracy, confusion matrices, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Try to locate processed parquet; fallback to sample CSV\n",
    "parquet_candidates = glob.glob(\"data/processed.*.parquet\")\n",
    "if len(parquet_candidates) > 0:\n",
    "    path = parquet_candidates[0]\n",
    "    print(\"Using processed parquet:\", path)\n",
    "    df = pd.read_parquet(path)\n",
    "else:\n",
    "    print(\"No processed.parquet found; falling back to data/sample_sanitized.csv\")\n",
    "    df = pd.read_csv(\"data/sample_sanitized.csv\")\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "df = df.dropna(subset=[\"full_text\", \"score\"])\n",
    "df[\"score\"] = df[\"score\"].astype(int)\n",
    "\n",
    "# Use a small subset for quick runs\n",
    "df_small = df.sample(n=min(1000, len(df)), random_state=42).reset_index(drop=True)\n",
    "X = df_small[\"full_text\"].astype(str).tolist()\n",
    "y = df_small[\"score\"].astype(int).tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Train size:\", len(X_train), \"Test size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.baseline.classical import TFIDFLogistic\n",
    "from src.eval.metrics import (\n",
    "    qwk,\n",
    "    within_one_accuracy,\n",
    "    confusion,\n",
    "    classification_report_dict,\n",
    ")\n",
    "from src.baseline.llm_client import MockLLMClient\n",
    "from src.baseline.prompt_runner import PromptRunner\n",
    "\n",
    "# Train TFIDF baseline\n",
    "clf = TFIDFLogistic(max_features=5000, ngram_range=(1, 2))\n",
    "clf.fit(X_train, y_train)\n",
    "preds_clf = clf.predict(X_test)\n",
    "\n",
    "print(\"TFIDF Logistic QWK:\", qwk(y_test, preds_clf))\n",
    "print(\"TFIDF Logistic ±1 acc:\", within_one_accuracy(y_test, preds_clf))\n",
    "\n",
    "cm_clf, labels = confusion(y_test, preds_clf, labels=sorted(set(y)))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "print(labels)\n",
    "print(cm_clf)\n",
    "\n",
    "print(\"\\nClassification report (TFIDF):\")\n",
    "import json\n",
    "\n",
    "print(json.dumps(classification_report_dict(y_test, preds_clf), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Mock LLM prompt runner on test set\n",
    "mock = MockLLMClient()\n",
    "runner = PromptRunner(llm_client=mock)\n",
    "llm_outputs = [runner.run_single(text) for text in X_test]\n",
    "preds_llm = [int(o[\"score\"]) for o in llm_outputs]\n",
    "\n",
    "print(\"\\nMock LLM QWK:\", qwk(y_test, preds_llm))\n",
    "print(\"Mock LLM ±1 acc:\", within_one_accuracy(y_test, preds_llm))\n",
    "cm_llm, _ = confusion(y_test, preds_llm, labels=sorted(set(y)))\n",
    "print(\"Mock LLM confusion matrix:\")\n",
    "print(cm_llm)\n",
    "\n",
    "print(\"\\nExample mock outputs (first 5):\")\n",
    "for i, out in enumerate(llm_outputs[:5]):\n",
    "    print(i, out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}